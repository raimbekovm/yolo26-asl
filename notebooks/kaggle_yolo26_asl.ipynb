{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO26 vs YOLO11: ASL Detection Benchmark\n",
    "\n",
    "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-raimbekovm/yolo26--asl-blue)](https://github.com/raimbekovm/yolo26-asl)\n",
    "\n",
    "This notebook benchmarks **YOLO26** vs **YOLO11** on American Sign Language (ASL) letter detection.\n",
    "\n",
    "## What We'll Do\n",
    "1. Download ASL dataset (26 letter classes)\n",
    "2. Train YOLO26n on ASL detection\n",
    "3. Train YOLO11n for comparison\n",
    "4. Benchmark speed and accuracy\n",
    "5. Export results\n",
    "\n",
    "**Author:** Murat Raimbekov  \n",
    "**License:** Apache 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q ultralytics>=8.3.0 roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "\n",
    "print(f\"Ultralytics: {ultralytics.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset\n",
    "\n",
    "We use the **American Sign Language Letters** dataset from Roboflow Universe.\n",
    "- 26 classes (A-Z)\n",
    "- ~700 images with bounding box annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "# Initialize Roboflow\n",
    "# Get your API key from https://roboflow.com/settings/api\n",
    "rf = Roboflow(api_key=\"YOUR_ROBOFLOW_API_KEY\")  # Replace with your key\n",
    "\n",
    "# Download dataset\n",
    "project = rf.workspace(\"david-lee-d0rhs\").project(\"american-sign-language-letters\")\n",
    "dataset = project.version(6).download(\"yolov8\")\n",
    "\n",
    "print(f\"Dataset downloaded to: {dataset.location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset structure\n",
    "import yaml\n",
    "\n",
    "data_yaml = f\"{dataset.location}/data.yaml\"\n",
    "with open(data_yaml) as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Classes: {data_config['nc']}\")\n",
    "print(f\"Names: {data_config['names']}\")\n",
    "\n",
    "# Count images\n",
    "train_imgs = len(list(Path(f\"{dataset.location}/train/images\").glob(\"*.jpg\")))\n",
    "val_imgs = len(list(Path(f\"{dataset.location}/valid/images\").glob(\"*.jpg\")))\n",
    "test_imgs = len(list(Path(f\"{dataset.location}/test/images\").glob(\"*.jpg\")))\n",
    "\n",
    "print(f\"\\nTrain: {train_imgs} images\")\n",
    "print(f\"Valid: {val_imgs} images\")\n",
    "print(f\"Test: {test_imgs} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train YOLO26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO26n model\n",
    "yolo26 = YOLO(\"yolo26n.pt\")\n",
    "\n",
    "print(f\"YOLO26n parameters: {sum(p.numel() for p in yolo26.model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLO26 on ASL dataset\n",
    "results_26 = yolo26.train(\n",
    "    data=data_yaml,\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name=\"yolo26n_asl\",\n",
    "    patience=20,\n",
    "    save=True,\n",
    "    plots=True,\n",
    "    device=0,  # GPU\n",
    "    workers=4,\n",
    "    # Augmentation\n",
    "    augment=True,\n",
    "    mixup=0.1,\n",
    "    mosaic=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate YOLO26\n",
    "yolo26_best = YOLO(\"runs/detect/yolo26n_asl/weights/best.pt\")\n",
    "metrics_26 = yolo26_best.val(data=data_yaml)\n",
    "\n",
    "print(f\"\\nYOLO26 Results:\")\n",
    "print(f\"  mAP50: {metrics_26.box.map50:.4f}\")\n",
    "print(f\"  mAP50-95: {metrics_26.box.map:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train YOLO11 (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO11n model\n",
    "yolo11 = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "print(f\"YOLO11n parameters: {sum(p.numel() for p in yolo11.model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train YOLO11 on ASL dataset (same settings)\n",
    "results_11 = yolo11.train(\n",
    "    data=data_yaml,\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name=\"yolo11n_asl\",\n",
    "    patience=20,\n",
    "    save=True,\n",
    "    plots=True,\n",
    "    device=0,\n",
    "    workers=4,\n",
    "    augment=True,\n",
    "    mixup=0.1,\n",
    "    mosaic=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate YOLO11\n",
    "yolo11_best = YOLO(\"runs/detect/yolo11n_asl/weights/best.pt\")\n",
    "metrics_11 = yolo11_best.val(data=data_yaml)\n",
    "\n",
    "print(f\"\\nYOLO11 Results:\")\n",
    "print(f\"  mAP50: {metrics_11.box.map50:.4f}\")\n",
    "print(f\"  mAP50-95: {metrics_11.box.map:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmark Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, num_runs=100, imgsz=640, device='cuda'):\n",
    "    \"\"\"\n",
    "    Benchmark model inference speed.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Timing results in milliseconds\n",
    "    \"\"\"\n",
    "    # Create dummy input\n",
    "    dummy = np.random.randint(0, 255, (imgsz, imgsz, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        model(dummy, verbose=False, device=device)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start = time.perf_counter()\n",
    "        model(dummy, verbose=False, device=device)\n",
    "        times.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    times = np.array(times)\n",
    "    return {\n",
    "        'mean_ms': np.mean(times),\n",
    "        'std_ms': np.std(times),\n",
    "        'min_ms': np.min(times),\n",
    "        'max_ms': np.max(times),\n",
    "        'fps': 1000 / np.mean(times)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark on GPU\n",
    "print(\"Benchmarking on GPU...\\n\")\n",
    "\n",
    "bench_26_gpu = benchmark_inference(yolo26_best, num_runs=100, device='cuda')\n",
    "bench_11_gpu = benchmark_inference(yolo11_best, num_runs=100, device='cuda')\n",
    "\n",
    "print(f\"YOLO26 (GPU): {bench_26_gpu['mean_ms']:.2f} ± {bench_26_gpu['std_ms']:.2f} ms ({bench_26_gpu['fps']:.1f} FPS)\")\n",
    "print(f\"YOLO11 (GPU): {bench_11_gpu['mean_ms']:.2f} ± {bench_11_gpu['std_ms']:.2f} ms ({bench_11_gpu['fps']:.1f} FPS)\")\n",
    "\n",
    "gpu_speedup = bench_11_gpu['mean_ms'] / bench_26_gpu['mean_ms']\n",
    "print(f\"\\nGPU Speedup: {(gpu_speedup-1)*100:.1f}% faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark on CPU\n",
    "print(\"Benchmarking on CPU...\\n\")\n",
    "\n",
    "bench_26_cpu = benchmark_inference(yolo26_best, num_runs=50, device='cpu')\n",
    "bench_11_cpu = benchmark_inference(yolo11_best, num_runs=50, device='cpu')\n",
    "\n",
    "print(f\"YOLO26 (CPU): {bench_26_cpu['mean_ms']:.2f} ± {bench_26_cpu['std_ms']:.2f} ms ({bench_26_cpu['fps']:.1f} FPS)\")\n",
    "print(f\"YOLO11 (CPU): {bench_11_cpu['mean_ms']:.2f} ± {bench_11_cpu['std_ms']:.2f} ms ({bench_11_cpu['fps']:.1f} FPS)\")\n",
    "\n",
    "cpu_speedup = bench_11_cpu['mean_ms'] / bench_26_cpu['mean_ms']\n",
    "print(f\"\\nCPU Speedup: {(cpu_speedup-1)*100:.1f}% faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['YOLO26n', 'YOLO11n'],\n",
    "    'mAP50': [metrics_26.box.map50, metrics_11.box.map50],\n",
    "    'mAP50-95': [metrics_26.box.map, metrics_11.box.map],\n",
    "    'GPU (ms)': [bench_26_gpu['mean_ms'], bench_11_gpu['mean_ms']],\n",
    "    'GPU (FPS)': [bench_26_gpu['fps'], bench_11_gpu['fps']],\n",
    "    'CPU (ms)': [bench_26_cpu['mean_ms'], bench_11_cpu['mean_ms']],\n",
    "    'CPU (FPS)': [bench_26_cpu['fps'], bench_11_cpu['fps']],\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"YOLO26 vs YOLO11 - ASL Detection Benchmark\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "models = ['YOLO26n', 'YOLO11n']\n",
    "map50 = [metrics_26.box.map50, metrics_11.box.map50]\n",
    "map5095 = [metrics_26.box.map, metrics_11.box.map]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, map50, width, label='mAP50', color='#2ecc71')\n",
    "axes[0].bar(x + width/2, map5095, width, label='mAP50-95', color='#3498db')\n",
    "axes[0].set_ylabel('mAP')\n",
    "axes[0].set_title('Accuracy Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# GPU Speed\n",
    "gpu_times = [bench_26_gpu['mean_ms'], bench_11_gpu['mean_ms']]\n",
    "colors = ['#e74c3c' if t > min(gpu_times) else '#2ecc71' for t in gpu_times]\n",
    "axes[1].bar(models, gpu_times, color=colors)\n",
    "axes[1].set_ylabel('Inference Time (ms)')\n",
    "axes[1].set_title('GPU Speed (lower is better)')\n",
    "for i, v in enumerate(gpu_times):\n",
    "    axes[1].text(i, v + 0.5, f'{v:.1f}ms', ha='center')\n",
    "\n",
    "# CPU Speed\n",
    "cpu_times = [bench_26_cpu['mean_ms'], bench_11_cpu['mean_ms']]\n",
    "colors = ['#e74c3c' if t > min(cpu_times) else '#2ecc71' for t in cpu_times]\n",
    "axes[2].bar(models, cpu_times, color=colors)\n",
    "axes[2].set_ylabel('Inference Time (ms)')\n",
    "axes[2].set_title('CPU Speed (lower is better)')\n",
    "for i, v in enumerate(cpu_times):\n",
    "    axes[2].text(i, v + 1, f'{v:.1f}ms', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('benchmark_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test images\n",
    "test_images = list(Path(f\"{dataset.location}/test/images\").glob(\"*.jpg\"))[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_path in enumerate(test_images):\n",
    "    # Predict with YOLO26\n",
    "    results = yolo26_best(str(img_path), verbose=False)\n",
    "    \n",
    "    # Plot\n",
    "    annotated = results[0].plot()\n",
    "    axes[idx].imshow(annotated[..., ::-1])  # BGR to RGB\n",
    "    axes[idx].axis('off')\n",
    "    \n",
    "    # Get prediction\n",
    "    if len(results[0].boxes) > 0:\n",
    "        cls_id = int(results[0].boxes.cls[0])\n",
    "        conf = float(results[0].boxes.conf[0])\n",
    "        cls_name = results[0].names[cls_id]\n",
    "        axes[idx].set_title(f\"Pred: {cls_name} ({conf:.0%})\")\n",
    "\n",
    "plt.suptitle('YOLO26 ASL Detection Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX for deployment\n",
    "yolo26_best.export(format=\"onnx\", imgsz=640, simplify=True)\n",
    "print(\"YOLO26 exported to ONNX\")\n",
    "\n",
    "# Save benchmark results\n",
    "results_df.to_csv('benchmark_results.csv', index=False)\n",
    "print(\"Results saved to benchmark_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model for HuggingFace\n",
    "import shutil\n",
    "\n",
    "output_dir = Path(\"asl_model\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "shutil.copy(\"runs/detect/yolo26n_asl/weights/best.pt\", output_dir / \"yolo26n_asl.pt\")\n",
    "shutil.copy(\"benchmark_results.png\", output_dir)\n",
    "shutil.copy(\"benchmark_results.csv\", output_dir)\n",
    "\n",
    "print(f\"\\nModel and results saved to: {output_dir}\")\n",
    "print(\"Download this folder to deploy on HuggingFace Spaces!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Results\n",
    "\n",
    "| Metric | YOLO26n | YOLO11n |\n",
    "|--------|---------|--------|\n",
    "| mAP50 | TBD | TBD |\n",
    "| mAP50-95 | TBD | TBD |\n",
    "| GPU Speed | TBD ms | TBD ms |\n",
    "| CPU Speed | TBD ms | TBD ms |\n",
    "\n",
    "### Key Findings\n",
    "- YOLO26 NMS-free architecture enables faster inference\n",
    "- Both models achieve high accuracy on ASL detection\n",
    "- CPU speedup is particularly significant for edge deployment\n",
    "\n",
    "### Links\n",
    "- **GitHub**: [raimbekovm/yolo26-asl](https://github.com/raimbekovm/yolo26-asl)\n",
    "- **HuggingFace Demo**: Coming soon\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Murat Raimbekov  \n",
    "**License:** Apache 2.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
